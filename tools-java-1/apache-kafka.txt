core java - kafka

What is Apache kafka?

-	Apache kafka is distributed streaming platform.

-	We can use kafka to create real time streams of data.

-	Also kafka can be used to process real time streams of data.

How does Apache kafka work?

-	Apache kafka uses publisher subscriber messaging system.

-	This system consists of three entities, called as publisher, subscriber and broker.

-	Publisher is also called as message producer.

-	It pushes message to queue also called as broker.

-	Subscriber is also called as message consumer.

-	It pulls message from the broker.

-	The publisher and subscriber are completely decoupled.

Explain difference between traditional messaging system and Apache kafka messaging system?

-	In traditional messaging system the message is not persisted, but Apache kafka persist the message for specific time period.

-	Traditional messaging system do not follow distributed architecture, but Apache kafka follow distributed architecture.

-	In traditional messaging system brokers keep track of consumed message, but in Apache kafka consumers keep track of consumed message.

-	Traditional messaging system allow only specific consumer to consume message, but Apache kafka allow any consumer subscribed to topic to consume the message.

Enlist the components of kafka ecosystem?

-	The components of kafka ecosystem are broker, client, connector, stream and ksql.

What is a broker?

-	Broker is kafka server or kafka queue.

-	It is a middle man between the producer and consumer.

-	Following command must be used to start broker.

	syntax:
	
	kafka-server-start.bat [configFilePath]

	example:
	
	kafka-server-start.bat ..\..\config\server.properties

What is a cluster?

-	A cluster is group of computers acting together for common purpose.

-	A kafka cluster is group of computers each running one instance of broker.

-	This ensures availability, fault tolerance, reliable work distribution, scalability and concurrency.

What is a zookeeper?

-	The zookeeper is responsible to manage all the brokers inside the cluster.

-	Following command must be used to start zookeeper.

	syntax:
	
	zookeeper-server-start.bat [configFilePath]

	example:
	
	zookeeper-server-start.bat ..\..\config\zookeeper.properties
	
What is a topic?

-	A topic is unique name given to group of message records.

-	Analogy with RDMBS, a topic within broker is similar to table within a database.

-	Following command must be used to create topic.

	syntax:
	
	kafka-topics.bat --create --topic [topicName] -zookeeper [ipAddress:portNumber] --replication-factor [replicationFactor] --partitions [partitionCount]
	
	example:
	
	kafka-topics.bat --create --topic my-topic-001 -zookeeper localhost:2181 --replication-factor 1 --partitions 4
	
-	Following command must be used to list all the topics present inside broker.

	syntax:
	
	kafka-topics.bat --zookeeper [ipAddress:portNumber] --list
	
	example:
	
	kafka-topics.bat --zookeeper localhost:2181 --list
	
-	Following command must be used to describe a specific topic present inside broker.

	syntax:
	
	kafka-topics.bat --zookeeper localhost:2181 --describe --topic [topicName]
	
	example:
	
	kafka-topics.bat --zookeeper localhost:2181 --describe --topic my-topic-001
	
What are topic partitions?

-	A single topic is broken into small parts which are distributed across cluster, these parts are called as partitions.

What is a producer?

-	Producer is application which pushes message record to the broker.

-	Client program will use apache kafka producer API to implement producer.

-	Following command must be used to create console producer.

	syntax:
	
	kafka-console-producer.bat --broker-list [ipAddress:portNumber] --topic [topicName]
	
	example:

	kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic-001

What is a consumer?

-	Consumer is application which pulls message record from the broker.

-	Consumer will request message record from broker.

-	Broker immediately will send message record it received from the producer to the consumer.

-	Consumer will process the message record and will request next message record from broker.

-	Client program will use apache kafka consumer API to implement consumer.

-	Following command must be used to create console consumer.

	syntax:
	
	kafka-console-consumer.bat --bootstrap-server [ipAddress:portNumber] --topic [topicName] --from-beginning
	
	example:

	kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic-001 --from-beginning

What is a partitioner?

-	A partitioner is the component inside broker responsible to determine the partition location of message record.

-	A producer can push message record to partition as plain value or in form of key and value pair.

-	When message record is simply plain value then partitioner push it inside any partition using round robin algorithm.

-	But when message record is in form of key and value pair then key undergoes hashing to generate hash value.

-	The hash value is used to determine the partition location of message record.

-	This way message records with same key value are pushed to same partition location.

-	Following command must be used to create console producer which uses key in message record.

	syntax:
	
	kafka-console-producer.bat --broker-list [ipAddress:portNumber] --topic [topicName] --from-beginning --property "key.separator=[value]" --property "parse.key=[value]"
	
	example:

	kafka-console-producer.bat --broker-list localhost:9092 --topic my-topic-001 --from-beginning --property "key.separator=-" --property "parse.key=true"
	
-	Following command must be used to create console consumer which uses key in message record.

	syntax:
	
	kafka-console-consumer.bat --bootstrap-server [ipAddress:portNumber] --topic [topicName] --from-beginning -property "key.separator=[value]" --property "print.key=[value]"
	
	example:

	kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic-001 --from-beginning -property "key.separator=-" --property "print.key=true"

What is partition offset?

-	A partition offset is sequence number for message record in the partition.

-	Broker is responsible to assign sequence number for every message record present in partition.

-	This number is unique within the partition and not unique within the topic.

-	A partition continues to grow as new message records are added.

-	All message records are persisted inside commit log file.

-	A consumer has three options to pull message records from partition.

-	It can read all message record from beginning, latest or from specific offset value.

What is consumer offset?

-	Whenever consumer pulls message record from the partition it makes note of the offset value of the last message record pulled.

-	This offset value is pushed as message record inside special topic called __consumer_offset.

-	Further consumer pulls the offset value from __consumer_offset topic to understand which next message record it must pull from the partition.
	
What is a consumer group?

-	A group of one or more consumer applications which share work load is called consumer group.

-	Apache kafka allows only one consumer application from the consumer group to pull message record from one partition.

-	This will also prevent duplicate pull of the same message record.

-	Consumer group is used for scalable message consumption.

-	Different application must have different consumer group identifiers.

-	Following command must be used to create consumer and adding it to consumer group.

	syntax:

	kafka-console-consumer.bat --bootstrap-server [ipAddress:portNumber] --topic [topicName] --group [groupName]
	
	example:
	
	kafka-console-consumer.bat --bootstrap-server localhost:9092 --topic my-topic-001 --group my-group-001
	
-	When consumer is created without group then it kafka will create new group and consume to that consumer group.	

-	Following command must be used to list all the consumer groups present inside broker.

	syntax:
	
	kafka-consumer-groups.bat --bootstrap-server [ipAddress:portNumber] --list
	
	example:
	
	kafka-consumer-groups.bat --bootstrap-server localhost:9092 --list

What is commit log?

-	The message records in kafka are maintained in form of file system inside broker.

-	The location of this file system can be configured using following property inside server.properties file.

	example:
	
	log.dir = /tmp/kafka-logs
	
-	Under this location we have multiple folders one for each partition present inside the broker.

-	The naming convention for these folders is [topicName-partitionCount].

-	Message records are maintained inside a file named as 0000000000000000.log under this folder.

-	These message records are present in form of bytes.

-	Following command must be used to read message record from 0000000000000000.log file.

	syntax:
	
	kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --files [filePath]
	
	example:
	
	kafka-run-class.bat kafka.tools.DumpLogSegments --deep-iteration --files /tmp/kafka-logs/my-topic-001/00000000000000000000.log

What is retention policy?

-	The retention policy determines how long in hours the message record will be present inside the partition.

-	It can be configured using following property inside server.properties file.

	example:
	
	log.retention.hours = 168
	
-	The default retention period is 168 hours i.e. 7 days.

How to create cluster?

-	To create cluster of three brokers we need three server.properties file inside config folder.

-	Following properties must be configured inside each server.properties file.

	example:
	
	broker.id=1
	
	listeners=PLAINTEXT://localhost:9091
	
	log.dirs=/tmp/kafka-logs-001

	auto.create.topics.enable=true

-	Following command must be used to start broker.

	syntax:
	
	kafka-server-start.bat [configFilePath]

	example:
	
	kafka-server-start.bat ..\..\config\server.001properties
	
Explain how topic is distributed across cluster?

-	All broker register themselve to zookeeper on start.

-	From the registered brokers zookeeper select one broker as controller.

-	Whenever zookeeper receives a command for creating new topic it forward instruction to controller broker.

-	The controller broker instructs the remaining brokers to create partition inside them.

-	When partition is created inside the broker then that broker is called as leader of that partition.

Explain how kafka handles data loss?

-	When partition is created inside the broker then that broker is called as leader of that partition.

-	However it is possible that leader broker may fail and message records present inside partition are lost.

-	This will lead to single point of failure.

-	To avoid single point of failure apache kafka uses replication strategy.

-	The entire partition present inside leader broker is copied into the other brokers.

-	These other set of brokers are called follower brokers.

-	This will ensure we have more than one copy of message records present inside each partition.

-	The number of copies of partition is decided by the replication factor parameter defined while creating topic.

-	Following command must be used to create topic.

	syntax:
	
	kafka-topics.bat --create --topic [topicName] -zookeeper [ipAddress:portNumber] --replication-factor [replicationFactor] --partitions [partitionCount]
	
	example:
	
	kafka-topics.bat --create --topic my-topic-001 -zookeeper localhost:2181 --replication-factor 1 --partitions 4

-	Whenever zookeeper identifies leader broker of a partition is failed it forward instruction to controller broker.

-	The controller broker then select one of the follower broker and make it the leader broker for that partition.

What is in-sync replica?

-	The in-sync replica represent the number of brokers which are synchronous with each other inside the cluster.

-	This include both leader and follower broker.

-	It is recommended to have in-sync replica always greater than one.

-	Ideal value for in-sync replica must be equal to replication factor.

-	Following property must be configured to define in-sync replica value.

	example:
	
	min.insync.replicas = 3
	
-	This property can be defined at broker or topic level.

Explain the layers through which message record must pass before it reaches the cluster?

-	At the producer end message record must pass through three layers before it reaches the cluster.

-	The first layer is serializer layer.

-	This layer is responsible to tranforming the message record into binary format.

-	This tranformation is performed on both message record's key and value.

-	Following properties can be used to define serialization strategy for message record.

	example:

	key.serializer
	
	value.serializer

-	Next layer is the partitioner layer.

-	The last layer is called as record batch accumulator layer.

-	This layer will contain multiple buffer one buffer for one partition present inside the cluster.

-	Before pushing the message records to cluster they are simply accumulated inside these buffers.

-	Once buffer limit reaches the upper threshold these message records are pushed to partition.

-	Following property can be used to define upper threshold value for each buffer.

	batch.size
	
-	Following property can be used to define upper threshold value as the sum of all the buffers.

	buffer.memory
	
-	Sometimes the producer does not produce enough message records and hence buffer limit does not reach the upper threshold.

-	In this case the message records are pushed to the partition after time duration.

-	Following property can be used to define time duration.

	linger.ms	

How to implement kafka producer in java?

-	To implement kafka producer we need to add the following mavena dependency inside pom.xml file.

	example:
	
	<dependency>
		<groupId>org.apache.kafka</groupId>
		<artifactId>kafka-clients</artifactId>
		<version>2.4.0</version>
	</dependency>
	
-	Initialize configuration properties for kafka producer.

	example:
	
	/**comma separated broker servers list**/
	propMap.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS_CONFIG);
	
	/**java class used for key serialization**/
	propMap.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
	
	/**java class used for value serialization**/
	propMap.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName());
	
-	Create instance of KakfkaProducer class.

	example:
	
	KafkaProducer producer = new KafkaProducer<String, String>(propMap);
	
-	Producer can push message record to the cluster synchronously and asynchronously.

-	When message record is pushed synchronously then producer thread wait until it passes through all layers and is written to the commit log file.

-	But when message record is pushed asynchronously then producer thread does not wait until it passes through all layers and is written to the commit log file.	

-	Asynchronous way of pushing message record is implemented using call back.
	
-	Push message record synchronously to the kafka broker.

	example:

	/**Wrapper class for message record before pushing to partition**/
	ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, msgKey, msgValue);
	
	/**Wrapper class for message record after pushing to partition**/
	RecordMetadata recordMetadata = producer.send(record).get();
	
-	Push message record asynchronously to the kafka broker.

	example:

	Callback callback = (recordMetadata, exception) {
		
	};

	ProducerRecord<String, String> record = new ProducerRecord<>(TOPIC_NAME, msgKey, msgValue);
	producer.send(record, callback);	
	
Explain important producer configurations?

-	Following are important producer configurations.

-	Acknowledgement configuration define when the producer should be informed about the message record status.

-	It is defined using following configuration property.	
	
	ack
	
-	Following are different possible values for the ack property.

	0	-	Consider success
	
	1	-	Consider success when message record is pushed to leader broker, this is the default value.

	ALL	-	Consider success when message record is pushed to leader broker and replicated to follower brokers.

-	Retries configuration define maximum number of attempts the producer must make to push message record in case of failure.

-	It is defined using following configuration property.	
	
	retries
	
-	This configuration can have integer value from 1 to 2147483647. The value 2147483647 is the default value.

-	Back off interval define time interval between two consecutive retries.

-	It is defined using following configuration property.	

	retry.backoff.ms
	
-	This configuration can have integer value. The value 100 is the default value.	

How to implement kafka consumer in java?

-	To implement kafka consumer we need to add the following mavena dependency inside pom.xml file.

	example:
	
	<dependency>
		<groupId>org.apache.kafka</groupId>
		<artifactId>kafka-clients</artifactId>
		<version>2.4.0</version>
	</dependency>
	
-	Initialize configuration properties for kafka consumer.

	example:
	
	/**comma separated broker servers list**/
	propMap.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS_CONFIG);
	
	/**java class used for key deserialization**/
	propMap.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
	
	/**java class used for value deserialization**/
	propMap.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
	
	/**consumer group name**/
	propMap.put(ConsumerConfig.GROUP_ID_CONFIG, GROUP_NAME);
	
-	Create instance of KafkaConsumer class.

	example:
	
	KafkaConsumer consumer = new KafkaConsumer<String, String>(propMap);
	
-	Consumer should subscribe to the topic.

	example:
	
	consumer.subscribe(Arrays.asList(TOPIC_NAME));
	
-	Pull message record from the kafka broker.

	example:

	while (true) {
		ConsumerRecords<String, String> records = consumer.poll(timeOutDuration);
	}

Explain important consumer configurations?

-	Following are important consumer configurations.

-	Offset reset property define how consumer should pull message records from the partition.

-	It is defined using following configuration property.	
	
	auto.offset.reset
	
-	Following are different possible values for the auto.offset.reset property.

	earliest	-	Pull message records from first offset.
	
	latest		-	Pull only new message records, this is the default value.
	
-	Poll intevral property define the time interval between two consecutive pull operations.	

-	It is defined using following configuration property.	
	
	max.poll.interval.ms

-	This configuration can have integer value. The value 3000000 is the default value.
	
Explain the working of consumer rebalance?

-	Whenever a consumer attempt to pull message records from broker the pull request reaches group coordinator.

-	The group coordinator is responsible to assign or revoke one or more partitions from consumer instances.

-	When there is no consumer instance for same consumer group already running then group coordinator will assign all partition to this consumer instance.

-	But when there is a consumer instance for same consumer group already running then group coordinator will revoke one or more but not all partitions from running consumer instance.

-	It will assign these one or more partitions to this consumer instance.

-	Also when the group coordinator does not receive any pull request from previously running consumer for time duration greater than max.poll.interval.ms duration.

-	Then group coordinator revoke all partitions from running consumer instance.  

-	It will assign these one or more partitions to another consumer instance which is running from the same consumer group.

Explain different consumer offset committing strategies?

-	There are two consumer offset committing strategies.

-	They are called as automatic committing and manual committing strategies.

Explain the automatic committing strategy for consumer offset?

-	In automatic committing strategy the client or consumer API is responsible to commit the offset.

-	No additional code needs to be written.

-	It is defined using following configuration properties.

	enable.auto.commit
	
	auto.commit.interval.ms
	
-	The first property enable.auto.commit define whether the consumer should automatically commit offset or not.

-	It accepts a boolean value. The value true is default value.

-	The next property auto.commit.interval define the time interval between two consecutive commit operations.	

-	It accepts a integer value. The value 5000 is default value.

-	Consider consumer pulls message record from partition.

-	But consumer fail before auto.commit.interval.ms duration and hence it did not automatically commit the offset.

-	After rebalancing the partition will be asssigned to another consumer instance from same consumer group.

-	This will cause the consumer instance to again pull the same message record from the partition.

Explain the manual committing strategy for consumer offset?

-	In manual committing strategy the developer needs to write code to commit the offset.

-	Under manual committing strategy there are two approaches.

-	They are called as synchronous commit and asynchronous commit.

-	When offset is pushed synchronously then consumer thread wait until it passes through all layers and is written to the commit log file.

-	Any fail commit is retried.

-	In synchronous commit we need to set following consumer configuration property.

	example:
	
	/**disable the offset automatic commit**/
	propMap.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE.toString());
	
-	Implement synchronous way of offset commit.

	example:

	if (records.count() > 0) {
		consumer.commitSync();
	}	

-	But when offset is pushed asynchronously then consumer thread does not wait until it passes through all layers and is written to the commit log file.	

-	Any fail commit is not retried.

-	In asynchronous commit we need to set following consumer configuration property.

	example:
	
	/**disable the offset automatic commit**/
	propMap.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE.toString());
	
-	Implement asynchronous way of offset commit.

	example:

	if (records.count() > 0) {
		consumer.commitAsync();
	}
	
How to commit a specific offset?

-	To commit a specific offset we need to set following consumer configuration property.

	example:
	
	/**disable the offset automatic commit**/
	propMap.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, Boolean.FALSE.toString());

-	Create map to store topic, partition and offset information.

	example:
	
	HashMap<TopicPartition, OffsetAndMetadata> offsetMap = new HashMap<>();

-	Add topic, partition and offset information to the map.

	offsetMap.put(new TopicPartition(record.topic(), record.partition()), new OffsetAndMetadata(record.offset() + 1, null));
	
-	Implement synchronous way of offset commit.

	example:

	if (records.count() > 0) {
		consumer.commitSync(offsetMap);
	}	

-	Implement asynchronous way of offset commit.

	example:

	if (records.count() > 0) {
		consumer.commitAsync(offsetMap);
	}		
	
What is consumer rebalance listener?

-	Consumer rebalance listener is used to do cleanup activity before one or more partitions are revoked from consumer instance during consumer rebalancing.

-	It can also be used after partition assignment.

-	We need java class that implements ConsumerRebalanceListener interface.

	example:
	
	public class MessageRebalanceListener implements ConsumerRebalanceListener {
	
	}

-	Then we need to override the following methods.

	public void onPartitionsRevoked(Collection<TopicPartition> partitions) {
	
	}
	
	public void onPartitionsAssigned(Collection<TopicPartition> partitions) {
	
	}
	
-	Configure consumer instance with listener instance.	

	example:
	
	consumer.subscribe(Arrays.asList(TOPIC_NAME), new MessageRebalanceListener());

Explain the seek behavior of consumer?

-	The KafkaConsumer class provide four different seek methods.

-	They are as follows.

	public void seekToBegining(Collection<TopicPartition> partitions)	
	
	public void seekToEnd(Collection<TopicPartition> partitions)
	
	public void seek(TopicPartition partition, long offset)	

	public void seek(TopicPartition partition, OffsetAndMetadata offset)	
	
-	The following method will always make consumer instance to pull message record from first offset of the partition.

	example:
	
	public void seekToBegining(Collection<TopicPartition> partitions)	

-	The following method will always make consumer instance to pull message record from latest offset of the partition.

	example:

	public void seekToEnd(Collection<TopicPartition> partitions)	

-	The following methods will always make consumer instance to pull message record from specific offset of the partition.

	example:

	public void seek(TopicPartition partition, long offset)	

	public void seek(TopicPartition partition, OffsetAndMetadata offset)
	
-	It is possible that consumer instance pull message record from an offset within partition.

-	After the message record is processed the consumer instance fail, hence offset is not committed.

-	After consumer rebalancing another consumer instance will pull the same message record.

-	There are two approaches to avoid this, first approach is to maintain a offset in external store like database or file system.

-	Another approach is to determine whether message record is duplicate or not before processing it.	

How to create custom serializer?

-	To create custom serializer we need to implement the Serializer interface.

	example:

	public class ItemSerializer implements Serializer<Item> {
	
	}
	
-	Then we need to override the following method.

	public byte[] serialize(String topic, Item item) {
	
	} 
	
How to create custom deserializer?

-	To create custom deserializer we need to implement the Deserializer interface.

	example:

	public class ItemDeserializer implements Deserializer<Item> {
	
	}
	
-	Then we need to override the following method.

	public Item deserialize(String topic, byte[] data) {
	
	} 